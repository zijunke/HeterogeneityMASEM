---
title: "CFA_OneFactor"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 4
    number_sections: yes
  html_document: default
  word_document:
    toc: yes
    toc_depth: '4'
    number_sections: yes
---

```{r setup, include=FALSE, echo = F}
require("knitr")
#knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = getwd())
opts_knit$set(root.dir = 'D:/Research/2023/CompareMASEM/CFA/OneFactor/')
```

# Load packages & set working directory & read in data
```{r}
library(matrixcalc);library(MASS);library(Matrix)
library(coda);library(R2OpenBUGS);library(metaSEM)

# Working directory
wd = 'D:/Research/2023/CompareMASEM/CFA/OneFactor/'
setwd(wd)
```

# Functions
```{r}
# vector to matrix
v2m <- function(vec,p,corr= T){
	M = matrix(0,p,p)
	M[lower.tri(M)] = vec
	M = M + t(M)
	if(corr==TRUE){
		diag(M) = 1
	}else{
		diag(M) = diag(M)/2
	}
	return(M)
}

# impute missing values in covariance / correlation matrices of each study
# to obtain a rough estimate of the covariance matrix of covariance / correlation matrix
# weighted average correlation
Mimpute <- function(R,N,missing){
	if(is.null(missing)){
		return(R)
	}else{
		na.pos = which(is.na(R),arr.ind = TRUE)
		mu.N = mean(N)
		Rbar = apply(R,2,mean,na.rm = TRUE)# Becker's mean r
		
		for(coli in unique(na.pos[,2])){
			id = na.pos[(na.pos[,2] == coli),1]
			R[id,coli] = Rbar[coli]
		}
		return(R)
	}
}

# change the coordinating system of a vectorized matrix to the coordinating system of 
# the original matrix
# e.g., from vS to S, the former uses one coordinate (vil), whereas the latter uses two (j,k).
Get.vi2jk <- function(p,diag.incl=FALSE,byrow=FALSE){
	A = matrix(1,p,p)
	if(diag.incl ==FALSE){
		pp = p*(p-1)/2
		vi2jk <- matrix(NA,pp,3)
		vi2jk[,3] <- 1:pp
		if(byrow == FALSE){
			vi2jk[,1:2] <- which(lower.tri(A)==1,arr.ind = TRUE)
		}else{
			vi2jk[,1:2] <- which(upper.tri(A)==1,arr.ind = TRUE)
		}
		colnames(vi2jk) = c('j','k','vi')
	}else{
		pp = p*(p+1)/2
		vi2jk <- matrix(NA,pp,3)
		vi2jk[,3] <- 1:pp
		if(byrow == FALSE){
			vi2jk[,1:2] <- which(lower.tri(A,diag = TRUE)==1,arr.ind = TRUE)
		}else{
			vi2jk[,1:2] <- which(upper.tri(A,diag = TRUE)==1,arr.ind = TRUE)
		}
		colnames(vi2jk) = c('j','k','vi')		
	}
	return(vi2jk)
}

# change the coordinating system of a matrix to the coordinating system of 
# the corresponding vectorized matrix
# e.g., from S to vS, the former uses two coordinates (j,k), whereas the latter uses only one (vil).
Get.jk2vi <- function(vi2jk,p,diag.incl=FALSE){
	jk2vi = matrix(0,p,p)
	jk2vi[vi2jk[,1:2]] = vi2jk[,3]
	if(diag.incl){
		jk2vi = jk2vi + t(jk2vi)
		diag(jk2vi) = diag(jk2vi)/2
	}else{
		pp = p*(p-1)/2
		jk2vi = jk2vi + t(jk2vi) + diag(rep(pp+1,p))
	}
	return(jk2vi)
}

jkvil <- function(p){
	vi2jk	= Get.vi2jk(p)
	j	= vi2jk[,1]
	k	= vi2jk[,2]
	vil	= Get.jk2vi(vi2jk,p)
	return(list(j=j,k=k,vil=vil))
}

# compute the covariance matrix of correlation matrix
# based on Steiger (1980)
Corr.Cov <- function(vR,N,index.list){
	nvR	= length(vR)
	vR	= c(vR,1)
	NvR.cov = matrix(NA,nvR,nvR)
	j = index.list$j
	k = index.list$k
	vil = index.list$vil

	for(vi in 1:nvR){
		NvR.cov[vi,vi] = (1-(vR[vi])^2)^2
	}
	for(vi in 1:(nvR-1)){
	for(vj in (vi+1):nvR){
		NvR.cov[vi,vj] = ((vR[vil[j[vi],j[vj]]]-vR[vi]*vR[vil[k[vi],j[vj]]])*(vR[vil[k[vi],k[vj]]]-vR[vil[k[vi],j[vj]]]*vR[vj])
		 +(vR[vil[j[vi],k[vj]]]-vR[vil[j[vi],j[vj]]]*vR[vj])*(vR[vil[k[vi],j[vj]]]-vR[vi]*vR[vil[j[vi],j[vj]]])
		 +(vR[vil[j[vi],j[vj]]]-vR[vil[j[vi],k[vj]]]*vR[vj])*(vR[vil[k[vi],k[vj]]]-vR[vi]*vR[vil[j[vi],k[vj]]])
		 +(vR[vil[j[vi],k[vj]]]-vR[vi]*vR[vil[k[vi],k[vj]]])*(vR[vil[j[vj],k[vi]]]-vR[vil[k[vi],k[vj]]]*vR[vj]))/2
		NvR.cov[vj,vi] <- NvR.cov[vi,vj]
	}
	}

	vR.cov = NvR.cov/(N)
	vR.cov = as.matrix(nearPD(vR.cov,posd.tol = 1e-5)$mat)
	return(vR.cov)
}

# Use average correlation vector to compute V_psi
Vj <- function(vR.bar,N,pp,Nstudy,index.list){

	mu.N = mean(N)
	S.vR.bar = Corr.Cov(vR.bar,mu.N,index.list)
	inv.S.vR.bar = solve(S.vR.bar)
	tau.vR = array(NA,dim = c(Nstudy,pp,pp))
	S.vR = array(NA,dim = c(Nstudy,pp,pp))
	for(i in 1:Nstudy){
		S.vR[i,,]<- S.vR.bar/N[i]*mu.N
		tau.vR[i,,] <- inv.S.vR.bar/mu.N*N[i]
	}	
	return(list(S.vR = S.vR,tau.vR = tau.vR))
}

# Use individual correlation vectors to compute V_psi
Vj2 <- function(vR.impute,N,pp,Nstudy,index.list){

	tau.vR = array(NA,dim = c(Nstudy,pp,pp))
	S.vR = array(NA,dim = c(Nstudy,pp,pp))
	for(i in 1:Nstudy){
		S.vR[i,,] = Corr.Cov(vR.impute[i,],N[i],index.list)
		tau.vR[i,,] <- solve(S.vR[i,,])
	}	
	return(list(S.vR = S.vR,tau.vR = tau.vR))
}

# generate data for meta-analytic CFA 
# the two-level model of OSMASEM is used
Gen.CFA.data <- function(Nstudy,mu.N,Model.list,p,missing,N=NULL){

	beta = Model.list$beta
	tau = Model.list$tau
	ind = Model.list$ind
	Z = Model.list$Z
	pp = Model.list$pp
	j = Model.list$j
	j10 = Model.list$j10
	k = Model.list$k
	k10 = Model.list$k10
	vil = Model.list$vil	

	# predicted SEM parameters
	coefM <- Z%*%t(beta)

	# predicted part of the true correlation vector for each study
	vPs = t(apply(coefM,1,function(x,pp,j,k,j10,k10,ind){
		r = rep(NA,pp)
		for(vi in 1:pp){
		  r[vi] = x[j[vi]]*x[k[vi]]+x[j10[vi]]*x[k10[vi]]*ind[vi]		
		}
		return(r)
	},pp=pp,j=j,k=k,j10=j10,k10=k10,ind=ind) )

	# true correlation vector for each study
	if(tau[1]>0){
	   vP = t(apply(vPs,1,function(x,tau,pp){
		r = rep(NA,pp)
		for(vi in 1:pp){ r[vi] = rnorm(1,x[vi],sd=tau[vi]) }
		return(r)
	   },tau=tau,pp=pp) )
	}else{ vP=vPs }

	# sample size for each study
	if(is.null(N)){
	  N <- rzinb(n =Nstudy, k =0.8, lambda=round(mu.N*0.2), omega = 0)
	  N <- N + round(mu.N*0.8)
	}

	# observed correlations
	vR = matrix(NA,Nstudy,pp)
	for(studyi in 1:Nstudy){
		Pm = v2m(vP[studyi,],p,T)
		Pm = nearPD(Pm,corr=T)$mat
		Ri = cor(mvrnorm(N[studyi],rep(0,p),Pm))
		vR[studyi,] = Ri[lower.tri(Ri)]	
	}

	#source(paste(wd,'RealData.R',sep=''))
	#vR = Make.Missing2(vR,missing,miss.rate,N)	# generate missing values
	return(list(j=j,k=k,vil=vil,pp=pp,N=N,vR=vR,Z=Z))
}

d4osmasem <- function(dsim){
	j = dsim$j
	vR = dsim$vR
	N = dsim$N
	Z = as.matrix(dsim$Z)

	p = max(j)	
	R.l = as.list(as.data.frame(t(vR)))
	Mat = lapply(R.l,function(x,p) v2m(x,p,T),p=p) 
	my.df = Cor2DataFrame(Mat,N,acov = 'weighted')
	my.df$data = data.frame(my.df$data,covariate=scale(Z[,1]),check.names = FALSE)
	return(my.df)
}

wbugs <-function(data,initsl,prm,mfn,
	nchains=1,niter=60000,nburnin=30000,nthin=1,wd,
	diagm){
# data: a named list of the data in the likelihood model for OpenBUGS
# initsl: a list with nchains elements; each element is a list of starting values
# prm: vector of names of the parameters to save
# mfn: the file name of the likelihood model for OpenBUGS
# diagm: name of the convergence diagnostic method; either 'Geweke' or 'Gelman'
# The function checks convergence every niter-nburnin iterations
	
	fit = bugs(data,initsl,prm,mfn,
		n.chains=nchains,n.iter=niter,n.burnin=nburnin,n.thin=1,
		debug=F,saveExec=T,working.directory = wd)

	for(tryi in 2:20){
		print(paste0('Iteration: ',tryi*(niter-nburnin)))
		fit.coda = read.openbugs(stem="",thin = nthin)
		del.id = na.omit(match(c('ppp'),varnames(fit.coda)))
		print(summary(fit.coda),3)
		if(diagm=='Geweke'){
			if(length(del.id)>0){
				tmp.conv = geweke.diag(fit.coda[,-del.id])[[1]]$z
			}else{ tmp.conv = geweke.diag(fit.coda)[[1]]$z }
			crit = (sum((abs(tmp.conv)>1.96),na.rm = T)==0)
		}else if(diagm=='Gelman'){
			if(length(del.id)>0){
				tmp.conv = gelman.diag(fit.coda)$psrf[-del.id,2]
			}else{ tmp.conv = gelman.diag(fit.coda)$psrf[,2] }
			crit = (sum((tmp.conv>1.1),na.rm = T)==0)
		}
		if(crit){
			print(tmp.conv)
			print(summary(fit.coda),3)	
			break
		}else{	
			fit = bugs(data,initsl,prm,mfn,
			n.chains=nchains,n.iter=niter-nburnin+1,n.burnin=1,n.thin=1,
			restart=T,saveExec=T,working.directory = wd)
		}
	}
	ppp.id = match('ppp',prm)
	sel = NA
	if(is.na(ppp.id)){
		nprm = length(prm)
		for(i in 1:nprm){ 
			sel = c(sel,grep(prm[i],rownames(summary(fit.coda)$quantiles)))
		}
	}else{
		prm = prm[-ppp.id]
		nprm = length(prm)
		for(i in 1:nprm){ 
			sel = c(sel,grep(prm[i],rownames(summary(fit.coda)$quantiles))) 
		}
	}
	sel = sel[-1]
	sel = unique(sel)
	
	if(is.na(ppp.id)){ est = round(summary(fit.coda)$quantiles[sel,'50%'],3)
	}else{
		est = round(c(summary(fit.coda)$quantiles[sel,'50%'],
		summary(fit.coda)$statistics['ppp','Mean']),3)
	}
	psd = round(summary(fit.coda)$statistics[sel,'SD'],3)
	if(diagm=='Geweke'){
		CIl = round(HPDinterval(fit.coda,prob = .95)[[1]][sel,1],3)
		CIu = round(HPDinterval(fit.coda,prob = .95)[[1]][sel,2],3)
	}else if(diagm=='Gelman'){
		fit.coda.l = do.call(rbind,fit.coda)
		HPDCI = HPDinterval(mcmc(fit.coda.l),prob = .95)
		CIl = HPDCI[sel,1]
		CIu = HPDCI[sel,2]
	}
	sel.muL = grep('mu.L',names(est))
	sel.sdL = grep('sd.L',names(est))
	CVl = round(est[sel.muL] - 1.28*est[sel.sdL],3)
	CVu = round(est[sel.muL] + 1.28*est[sel.sdL],3)
	
	conv = round(c(tryi,tmp.conv),3)
	return(list(est=est,psd=psd,CIl=CIl,CIu=CIu,CVl=CVl,CVu=CVu,conv=conv,
		DIC=fit$DIC,fit.coda=fit.coda))
}
```

# BMASEM
## Data preparation
```{r}
## Exclude studies that did not report bivariate correlations
index <- Gnambs18$CorMat==1
Gnambs18 <- lapply(Gnambs18, function(x) x[index])

# Convert correlation matrices to correlation vectors
mR = Gnambs18$data
vR = sapply(mR,function(x){	x = x[c(1,3,4,7,10,2,5,6,8,9),c(1,3,4,7,10,2,5,6,8,9)] 
	return(x[lower.tri(x)])	})
vR = t(vR)

N      = Gnambs18$n # sample sizes within primary studies
mu.N   = mean(N) # mean sample size
Nstudy = length(Gnambs18$data) # the number of primary studies
Ninv   = 1/N # reciprocals of sample sizes

# Coordinates of correlation matrices and vectors
p  = 10	# number of variables
pp = p*(p-1)/2	# number of bivariate correlations 
index.list = jkvil(p)
j = index.list$j
k = index.list$k
vil = index.list$vil

# Covariance matrices of sample correlation vectors
vR.bar = apply(vR,2,mean,na.rm = TRUE)
Stau.vR = Vj(vR.bar,N,pp,Nstudy,index.list)
tau.vR = Stau.vR$tau.vR 

# information for the additional error term
mu.vR.psi = rep(0,pp)
df.prelim = 100*pp/mu.N+pp
alpha.prior.vE = (df.prelim-pp+1)/2
beta.prior.vE = alpha.prior.vE*(0.3/mu.N)

# Matrices for computing ppp
# Compute the between-study covariance matrix of true study-specific correlation vectors
# Z: First derivative of study-specific correlation vectors with respect to model 
#    parameters (factor loadings)
# NA: for Openbugs to replace with parameter estimates
# The vi_th element in the vectorized correlation matrix corresponds to the 
# correlation between the j_th and the k_th items. 
# In the bifactor model, the correlation between the j_th and the_kth items
# equals the product of the j_th and the_kth 
# factor loadings plus the product of the (j+10)_th and the (k+10)_th factor 
# loadings (the factor loadings of the method factors) if the two items are 
# loaded on the same method factor. Therefore, the first derivative of the vi_th 
# correlation equals a nonzero value when the derivative is with respect to the 
# j(+10)_th or the k(+10)_th factor loading and zero when it is with 
# respect to other SEM parameters 
Z <- matrix(0,pp,p)
for(vi in 1:pp){	Z[vi,c(j[vi],k[vi])] = NA	}	
# Diagonal covariance matrix of study-specific model parameters (factor loadings)
# Random factor loadings are assumed to be uncorrelated
V.theta = matrix(0,10,10)
diag(V.theta) = NA
```

## Model fitting
```{r}
data<-list("Nstudy","N","Ninv","mu.N",'p',"pp","j","k",'V.theta',"vR",
  "tau.vR",'Z',"mu.vR.psi",'alpha.prior.vE','beta.prior.vE')  # data

initsl <- list(list(mu.L=rep(.6,p),sd.L = rep(0.1,p),
  tau.R=mu.N*3,vR.psi = matrix(0,Nstudy,pp),vR.rep = vR))# Initial values

prm = c('mu.L','sd.L','ppp') # parameters to save
model.fn = paste(wd,'CFARandom.txt',sep='') # model file name

# stop every 10000 iterations to check whether convergence is achieved
fit = wbugs(data,initsl,prm,model.fn,
		nchains=1,niter=60000,nburnin=30000,nthin=1,wd,diagm='Geweke')
fit[-9]
```

# OSMASEM
## Data preparation
```{r}
# Modified based on the code from Jak & Cheung (2019) 
# Exclude studies that reported CFA results only
index <- Gnambs18$CorMat==1
Gnambs18 <- lapply(Gnambs18, function(x) x[index])

## Create a dataframe with the data and the asymptotic variances and covariances (acov)
my.df <- Cor2DataFrame(Gnambs18$data, Gnambs18$n, acov = "weighted")

## Add the standardized individualism as the moderator
## Standardization of the moderator improves the convergence.
my.df$data <- data.frame(my.df$data,
                         Individualism=scale(Gnambs18$Individualism),
                         check.names=FALSE)
summary(my.df)
```

## Model fitting
```{r}
## Specify the bifactor model
model0 <- "SE =~ p1*I1 + p3*I3 + p4*I4 + p7*I7 + p10*I10 +
                 n2*I2 + n5*I5 + n6*I6 + n8*I8 + n9*I9"

RAM0 <- lavaan2RAM(model0, obs.variables = paste0("I", 1:10),std.lv = TRUE)

## Create matrices with implicit diagonal constraints
M0 <- create.vechsR(A0=RAM0$A, S0=RAM0$S, F0=RAM0$F)

## Create heterogeneity variances
T0 <- create.Tau2(RAM=RAM0, RE.type="Diag", Transform="expLog", RE.startvalues=0.05)

## Fit the bifactor model with One-Stage MASEM
fit0 <- osmasem(model.name="No moderator", Mmatrix=M0, Tmatrix=T0, data=my.df)
summary(fit0, fitIndices= T)

## SRMR
osmasemSRMR(fit0)
```